<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Pengfei Yu" />

  
  
  
    
  
  <meta name="description" content="Considering language models as random variables brings insights into the challenges in LLMs and the mechanisms of various augmentation methods to improve LLMs, such has hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation and many other things..." />

  
    <link rel="alternate" hreflang="zh" href="https://perfec-yu.github.io/zh/post/getting-started/" />
  
  <link rel="alternate" hreflang="en-us" href="https://perfec-yu.github.io/post/getting-started/" />

  
  
  
    <meta name="theme-color" content="#EF525B" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d3a6035256cd210d1a6f1b5d1e6dcec6.css" />

  



  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://perfec-yu.github.io/post/getting-started/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="于" />
  <meta property="og:url" content="https://perfec-yu.github.io/post/getting-started/" />
  <meta property="og:title" content="Thoughts on Randomizing Lanugage Modeling for Better Language Learning | 于" />
  <meta property="og:description" content="Considering language models as random variables brings insights into the challenges in LLMs and the mechanisms of various augmentation methods to improve LLMs, such has hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation and many other things..." /><meta property="og:image" content="https://perfec-yu.github.io/post/getting-started/featured.jpg" />
    <meta property="twitter:image" content="https://perfec-yu.github.io/post/getting-started/featured.jpg" /><meta property="og:locale" content="en-us" />
  
    
    
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://perfec-yu.github.io/post/getting-started/"
  },
  "headline": "Thoughts on Randomizing Lanugage Modeling for Better Language Learning",
  
  "image": [
    "https://perfec-yu.github.io/post/getting-started/featured.jpg"
  ],
  
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Pengfei Yu"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "于",
    "logo": {
      "@type": "ImageObject",
      "url": "https://perfec-yu.github.io/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Considering language models as random variables brings insights into the challenges in LLMs and the mechanisms of various augmentation methods to improve LLMs, such has hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation and many other things..."
}
</script>

  

  

  

  





  <title>Thoughts on Randomizing Lanugage Modeling for Better Language Learning | 于</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="279b9966ca9cf3121ce924dca452bb1c" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.c0e755ce3b74e9dac504d2668ff9235e.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">于</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">于</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#hobby"><span>Hobbies</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        
        <li class="nav-item dropdown i18n-dropdown">
          <a href="#" class="nav-link " data-toggle="dropdown"
             aria-haspopup="true" aria-label="Languages">
            <i class="fas fa-globe mr-1" aria-hidden="true"></i></a>
          <div class="dropdown-menu">
            <div class="dropdown-item dropdown-item-active">
              <span>English</span>
            </div>
            
            <a class="dropdown-item" href="https://perfec-yu.github.io/zh/post/getting-started/">
              <span>中文 (简体)</span>
            </a>
            
          </div>
        </li>
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Thoughts on Randomizing Lanugage Modeling for Better Language Learning</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Pengfei Yu</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jan 1, 0001
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    17 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/ideas/">Ideas</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="why-writing-this-and-what-this-is-about">Why Writing This and What This Is About</h2>
<p><strong><em>TLDR</em></strong>: Why? Call for broader discussion and potential collaboration to improve and implement this idea. What? About building a new framework that is not based on a constant language model, but a stochastic language model. This new framework explains a lot challenges and recent work on LLMs and provides insights on improving langauge learning. <a href="#overview">Then click there to the main content</a></p>
<p>I recently came up with notion of modeling language not as a constant language model, but a random variable or a stoachastic language model. This idea originates from many things I have worked on, am working on and have read about. Also, it is not a completely new idea, instead it is related, similar to, and combines several established methods and mathematical models, though with minor differences to each of them. In fact I wouldn&rsquo;t even expect any framework that completely negates everything intelligent researchers have built for AI and machine learning. Anyway, I have been trying to formulate this stochastic language model and see what kind of insights we can get from this perspective. Interestingly, as I am developing this framework, I notice this is closely related a lot challenges or methodlogies in current LLM research, including but not limited to hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation, mixture-of-experts and multimodality (the connection between randomizing a language model and multimodality sounds a little weird, but I will explain later).</p>
<p>Currently, I have sort out the most basic concepts of what it means to randomize a language model, what are its implications&amp;benefits and roughly how we should build this model or improve existing LLMs by approximating this model. However, I found that it is not trivial to explain this idea to others in a short, e.g., 30min to 1h, period of time and using more brief media of communications such as slides. I was only able to convey and improve most of my thoughts through multiple conversations and active discussions with some of my collegue PhD students. However, due to the level of generality as it seems to me for now, I think this notion of stochastic language modeling is kind of useful for reaserch in LLMs. Moreover, limited by my personal mathematical capability and available computational and data resources, it seems very challenging to fully implement everything I am thinking of under this framework. Moreover, I think many on-going research in the community could be closely related to this framework. So I think maybe it is simply better to write about this idea for broader discussions, suggestions and potential collaboration to further improve this idea and bring this idea to reality. Another good thing about a post is that it doesn&rsquo;t have to like research papers or a set of slides that need to follow certain structures. I feel more comfortable talking about my thoughts in this style.</p>
<p>In this post, I will try my best, although it might be kind of hard, to present this idea in an organized way. It might happen that you get a little bit lost at the start, but if you keep reading, it is very possible at some point that at some point you realize &ldquo;oh this is related to/ kind of explains what I am working on / have worked on&rdquo;. I would be very happy if these kind of situations happen and if you are interested in further discussion on this idea, please reach out at pengfei4 at illinois dot edu.</p>
<h2 id="overview">Overview</h2>
<p>I will try to explain this idea in the following aspects.</p>
<ol>
<li>Basic mathematical definition of the stochastic language modeling. I shall use the name <strong>Stochastic Procedural Language Modeling</strong> or SPLM since it is better to explain as a stochastic process rather than a single random variable. I know it could be elusive and boring to start with math rather than vivid examples, but I found that even a basic notion of what we are aiming for will benefit the understanding the &ldquo;vivid examples&rdquo;. The math part will not go beyond undergraduate-level probability theory, since I will not go into deeper analysis of the stochastic process. The purpose is just to leave a basic impression of the objective.</li>
<li>How this framework gives better explaination of various challenges mentioned above in LLMs, and how it is potentially easier to solve such challenges under this framework.</li>
<li>Explaning that a series of augmentation methods proposed for LLM are trying to approximate this framework. Moreover, how this framework explains an incomplete collection recent work.</li>
<li>Some initial thoughts on how to implement this idea, and how flexible this implemented framework will be in representing not only language but many intelligent behaviors. (to-be finished)</li>
</ol>
<h2 id="definition-of-stochstic-procedural-language-model">Definition of Stochstic Procedural Language Model</h2>
<h3 id="conventions-for-notations">Conventions for Notations</h3>
<ul>
<li>Lower case letters for variables</li>
<li>Upper letters for mappings (functions, distributions, etc.)</li>
<li>Caligraphic letters for sets or spaces, except for conventional notations such as $\mathbb{R}$ or $[0,1]$.</li>
<li>Greak letters for random variables</li>
</ul>
<h3 id="list-of-notations">List of Notations</h3>
<ul>
<li>$\mathcal{V}$: vocabulary, or set of tokens</li>
<li>$\mathcal{S}=\bigcup_{n=1}^{\infty} \mathcal{V}^n$: set of textual sequences</li>
<li>$\mathcal{P}_\mathcal{S}$: the space of all probability distributions on $\mathcal{S}$ (language models)</li>
<li>$\Pi_\mathcal{S}=\mathcal{P}\circ\mathcal{P}_\mathcal{S}$: the space of distributions over language models (stochastic language models)</li>
</ul>
<p>If you just want the conclusion or you get lost while reading, go to <a href="#summary-of-mathematics">conclusion</a>.</p>
<h3 id="stochstic-procedural-language-model">Stochstic Procedural Language Model</h3>
<p>A stochastic language model, is simply a random variable $\mu\in\Pi_\mathcal{S}$. A stochastic process is used to model the process of generating a textual sequence under a stochastic language model. This is the reason why I think for language modeling, Stochstic Procedural Language Modeling is a better name. Before I illustrate the process, I would like to share some very simple intuitions and thoughts on adding the randomness.</p>
<p>For a language model $M\in\mathcal{P}_\mathcal{S}$, it gives a probability $P(s|M)$ for any textual sequence $s\in\mathcal{S}$. However, such a constant language model to me is like a &ldquo;dead&rdquo; model that doesn&rsquo;t accept any more changes. BTW, from my understanding, this is also the reason why it is hard to even formulate an objective for continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF. The cross-entropy loss is not the real objective, since convergence to this loss on the fine-tuning data would result in catastrophic forgetting. In most cases we can just define some heuristic criteria to stop the fine-tuning. Adding additional regularization terms such as $\lambda ||\theta-\theta_0||_2^2$ to the cross entropy might serve as a valid objective, while acute readers might already see this is close to considering language model as a &ldquo;Gaussian language model&rdquo; centered at the pretrained state.</p>
<p>Another two similar things that one might think of are mixture of experts or VAE. Firstly for mixture of experts which typically works on finite components, the space of language models might not only be too large, but also uncountable (meaning it cannot be indexed by natural numbers in case someone may be unfamiliar with this concept. Set of even numbers is countable but set of real numbers is not.). VAE is indeed more related. In fact I think adpating the ELBO from VAE is one possible way of learning such a model. And apart from the apparently much larger latent space than VAEs we used to see in practice, there is something else fundamentally different as I will show after explaining the stochastic process of generation.</p>
<p>For stochastic process of generation, it can simply be understood as a sequence $(\xi_i,\mu_i), i\in[1,n]$.  Here each $\xi_i$ corresponds to the random variable of a generated token, and $\mu_i$ is the i-th step language model. In this process, a stochastic language model $\mu$ essentially serve as some sort of prior. From the theory of VAE $\mu_i$ can be encoded using the entire sequence for learning. But for now we can simply ignore everything about learning and simply formulate the generation process as
$$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)\hat{P}(\mu_i|\xi_{1:i-1}, \mu).
$$
There are actually a few alternative ways of decomposing this probabilities and this is just one of them. I choose this one because it aligns best with my understanding of how human compose sentences. I would like to add a few notes here before proceeding.</p>
<ul>
<li>I added a hat in $\hat{P}(\mu_i|\xi_{1:i-1}, \mu)$. Mathematically, $P(\mu_i|\xi_{1:i-1}, \mu)$ can be computed as a posterior once the prior $\mu$ is learned. However, I think this posterior is only a model for the observed language, not for the generation that serves certain purpose or fulfills certain tasks. This is also the key difference between this process and VAE. Human are not parrots that replicates the distribution it oberserves, but rather produce contents consistent with themselves&rsquo; values, beliefs, habits, knowledge and personalities etc. I will discuss how to get $\hat{P}$ later.</li>
<li>$P(\xi_i|\xi_{1:i=1}, \mu_i)$ discards the beautiful markov property and requires to re-compute everything when switching to a new language model. This might looks redundant at first glance but it will be explained.</li>
<li>For learning, we already see that $\mu$, the prior, needs also be learned rather than taking standard Gaussian as the original VAE. For example, we can take $\mu$ as a mixture of Gaussian and use a two-level EM (outer level for MoG and inner level for VAE).</li>
</ul>
<p>Since now $\hat{P}(\mu_i|\xi_{1:i-1}, \mu)$ is not the probability of $P(\mu_i|\xi_{1:i-1}, \mu)$, we shall consider it to be generated by another model (encoder in VAE) and rewrite $P(\mu_i|\xi_{1:i-1}, \mu, G)$ where $G$ is the new model. Now, a SPLM is simply defined as a pair of $(\mu, G)$, If we look closer at what $G$ is doing, it associates some text $\xi_{1:i-1}$ with a stochastic language model. From the derivation of VAE (or EM, KMeans or just intuition, whichever is best for you), $G$ is, very probably, grouping a set of text $\mathcal{T}$ together into a conditional stochastic language model $\mu_\mathcal{T}$. One possible interpretation of $\mu_\mathcal{T}$ would be <strong><em>a stochastic language model that is aware of the knowledge contained in $\mathcal{T}$</em></strong>. This is already one desired property that is not so clear in LLMs: we can <strong><em>pinpoint knowledge</em></strong> of a model into a latent stochastic LM, and moreover, the knowledge we are using at each step during generation can be interpreted.</p>
<p>Now, I will simply incorporte the constant language model into this framework as the last step of this section. Let $M=\mathbb{E}\mu$ (treat $\mu$ as an infinite-dimensional random vector and expectation is taken element-wise), and $G$ is a function that always produces a delta-distribution (one-hot) at $M$. In other words, $M$ is simply the expectation of the stochastic language model. What is bad about expectation? The most straightforward outcome is <strong><em>hallucination</em></strong>. It can be better illustrated with a non-linguistic example. Consider a ball falls uniformly on $x=-1$ or $x=1$, the expected position would be $x=0$ where the ball will never falls on. Similarly, a SPLM might pick either $\phi$ or $\psi$ for the next generation, but never an average of them. One last comment I would like to leave is that, since we observe that LM is just a first-order approximation of the SPLM, adding any higher-order momentums or information about their joint distributions would help. If you simply expand the higher-order moments with infinite-dimensional random vector view of the distributions, higher-order momentums can been seen as modeling some multi-concept relations or multiary logical rules. We can do further analysis on this process or add additional assumptions but I think it is good enough to stop here and show some concrete examples. But before that, I will summarize the math content in case you lost somewhere in the middle.</p>
<h3 id="summary-of-mathematics">Summary of Mathematics</h3>
<p>A SPLM is a pair of $(\mu, G)$. $\mu$ is some prior distribution of stochastic language models. Intuitively, it can contain several clusters. $G$ is a function that associates a group of text with another cluster of language models that are aware of the knowledge contained in the group of text, while this association can depend on some values or preferences we set to the model. For language modeling (evaluation of probabilities),
$$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)P(\mu_i|\xi_{1:i-1}, \mu),
$$
$G$ is ignored since the values we set doesn&rsquo;t reflect the natural language distribution. $P(\mu_i|\xi_{1:i-1}, \mu)$ can be estimated through Bayesian inference. For generation,
$$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)P(\mu_i|\xi_{1:i-1}, G, \mu).
$$</p>
<h2 id="splm-and-challenges-of-llms">SPLM and Challenges of LLMs</h2>
<p>We already see how SPLM supports better knowledge localization and reduced hallucination from the math part. I will list some other topics that people around or me are working on. Everyone is welcome to contribute to this list. For the following sections, I will explain more on some topics I am more familiar with and less on those I only have basic knowledge or are overly simple.</p>
<h3 id="modeling-deterministic-rulesbias">Modeling Deterministic Rules/Bias</h3>
<p>From my perspective, the cause of failure in LLMs in these two topics are essentially the same as hallucination: the averaging effect of taking expectation. In SPLM, deterministic rules might be modeled into a separate language model where there are a lot of $P(x|y)=1$ and it is simply a latent component of $\mu$. The function $G$ will call this component when we need to do deterministic reasoning. Similarly, we can set preferences in $G$ between the biased and unbiased components for debiasing. One might think that, since we want an unbiased model, isn&rsquo;t it equivalent to directly learn the induced LM from a $G,\mu$ when $G$ is set with the desired preference? There are several considerations here. Firstly, LLMs doesn&rsquo;t explicitly model the latent components, so training the LLM to imitate an unbiased distribution may destruct other knowledge or the logical consistency of the LLM. To be more straightforward consider $A$ is a biased opinion and $B$ refers to biased people. $P(A)=P(A|B)P(B)+P(A|\neg B)P(\neg B)$. If we simply train the model to have $P(A)=0$, the LLM could, unrigorsly speaking,</p>
<ul>
<li>forms the belief that biased people are no longer biased $P(A|B)=0$;</li>
<li>forms the belief that is logically inconsistent (breaking the above equality): even though there is biased speech from biased people, there is no biased speech.</li>
</ul>
<p>I personally believe the second cases should happen more frequently. In fact this is somewhat validated by the observed ripple effect in the next part for knowledge editing.</p>
<h3 id="ripple-effect-in-knowledge-editing-and-fine-tuning">Ripple Effect in Knowledge Editing and Fine-tuning</h3>
<p>Ripple effect refers to the effect of editing one fact might affect other facts or create new facts. For example,</p>
<ul>
<li>Fact: Messi has won a total of 7 Ballon d’Or b by 2022.</li>
<li>Knowledge Edit: Messi won Ballon d&rsquo;Or for 2023</li>
<li>New Fact: Messi has won a total of 8 Ballon d&rsquo;Or by 2023.</li>
</ul>
<p>I personally think this is not only a problem for editing or fine-tuning, but for training (including pre-training) as a whole. Language models, even GPT-4, display weak ripple effect throughout the learning process. I speculate this to originates from the failure of modeling higher-order features of language models being a first order approximation of stochastic language models, which is related to the joint distribution of $P(P(x|\mu), P(y|\mu))$ for related (or even unrelated) facts $x,y$. Note that since $\mu$ is a random variable, $P(x|\mu)$ is also a random variable. So the above distribution is also a &ldquo;distribution of distribution&rdquo;. Further analysis would require more math so I will just stop here for now. From an intuitive perspective, training SPLM for new knowledge involves encoding new knowledge to corresponding clusters that represent a group of text. So the effect naturally propagates to relevant facts. Below are two more complex examples of ripple effect in GPT-4. The left column show some knowledge it knows and right column shows that it cannot use the knowledge for other tasks.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Ripple Effect of GPT-4" srcset="
               /post/getting-started/ripple_1_hub4abfb4e90ee4a162fc9036c24ba2b14_350485_484d8320a907c6c3b0259955de46525f.webp 400w,
               /post/getting-started/ripple_1_hub4abfb4e90ee4a162fc9036c24ba2b14_350485_9be75e54b576e1f9f3a6887e64b65b52.webp 760w,
               /post/getting-started/ripple_1_hub4abfb4e90ee4a162fc9036c24ba2b14_350485_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/post/getting-started/ripple_1_hub4abfb4e90ee4a162fc9036c24ba2b14_350485_484d8320a907c6c3b0259955de46525f.webp"
               width="760"
               height="285"
               loading="lazy" data-zoomable /></div>
  </div></figure>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Ripple Effect of GPT-4" srcset="
               /post/getting-started/ripple_2_hu05783c26468b1f1bcb437b0b368bd3c3_177036_3e0d7690c920d09dd7d06b9f44a363b0.webp 400w,
               /post/getting-started/ripple_2_hu05783c26468b1f1bcb437b0b368bd3c3_177036_b411ef1a76eae7a325c2a3f6f19a7250.webp 760w,
               /post/getting-started/ripple_2_hu05783c26468b1f1bcb437b0b368bd3c3_177036_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/post/getting-started/ripple_2_hu05783c26468b1f1bcb437b0b368bd3c3_177036_3e0d7690c920d09dd7d06b9f44a363b0.webp"
               width="760"
               height="204"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="formulating-continual-fine-tuning">Formulating Continual Fine-tuning</h3>
<p>Actually I am not knowledgeable enough about the continual fine-tuning of EM algorithms, such as whether it can also be formulated as the converging point under certain algorithms. But at least, I think new examples can first be encoded into the space of stochastic language models, and if they appears to be a new cluster that is far away from existing clusters, we need to assign a new component for them.</p>
<h3 id="multimodality">Multimodality</h3>
<p>Note that the encoding of a sample refers to mapping a sample into a latent cluster of stochastic language models. For a multimodal model, this provides a more direct way of examining related concepts in various modalities (mapping to the same cluster).</p>
<h3 id="mappings-among-models">Mappings Among Models</h3>
<p>In SPLMs, we only need to map latent clusters to align two different models. Such alignment can be achived by using a set of sentences as test bed to see the probabilities of corresponding clusters on two models given each context.</p>
<h2 id="augmentation-methods-and-recent-work-that-can-be-explained-with-splm">Augmentation Methods and Recent Work that Can be Explained with SPLM</h2>
<h3 id="tool-learning-and-retrieval-augmentation">Tool learning and Retrieval Augmentation</h3>
<p>Both of these two popular methods have the following format: for a sequence splitted into two parts, $x_{1:m};x_{m+1:n}$, a function produces some additional information from $F(z|x_{1:m})$, and the model generates the rest part with $P(x_{m+1:n}|x_{1:m},z)$. There are two ways of understanding this framework.</p>
<ul>
<li>$G=F$ and $P(|z)$ is a sampled $\mu_n$;</li>
<li>$F$ is simply a sampled latent stochastic language model
There is no essential difference between this two, because the second one is equivalent to setting certain preference to $G$ under certain context. Under the first understanding, we can simply fine-tune $G$ (which i assume to be smaller since it is only a planner with limited knowledge). Under the second, it is actually more interesting. Since tools or retrievers can be considered as functions, we can simply get a set of input output pairs, $(x,y)$ and see if any existing cluster in $\mu$ already models this behavior. If not, we can train to incorporate a new cluster (function). Furthermore, if we firmly believe that a tool is more accurate than trained models (e.g., calculator), we can always call the tool when $G$ produces the new cluster. In essence, we are able to map almost any functions into the parameter space of the $\mu$.
PS: Look at this <a href="https://arxiv.org/abs/2104.09841" target="_blank" rel="noopener">paper</a> for retrieval augmentation. I already remember a simiar tool learning paper like this but cannot retrieve the name from my memory lol.</li>
</ul>
<h3 id="role-playing-and-other-smart-prompting-cot-in-context-learning">Role Playing and Other Smart Prompting (CoT, In-context Learning)</h3>
<p>Simply put, $P(|z)$ is a sampled $\mu_n$ for the prompt $z$, no matter inserted at the beginning or in the middle. A little more notes for in-context learning: it can be considered as forming a new cluster with the knowledge of provided examples.</p>
<h3 id="alignment-methods-sft--rlhf">Alignment Methods (SFT &amp; RLHF)</h3>
<p>Corresponds to fine-tuning $G$. However, in SPLM training $G$ will definitely not affect the knowledge structure (similar to the example explained in the debiasing), since knowledge is stored in $\mu$.</p>
<h3 id="some-recent-work">Some Recent Work</h3>
<p>This is definitely an incomplete list of work that is related to this framework. I just kind of randomly picked a few since this post is already too long. Also since I already relates this framework with many approaches, I am only showing those work that are not directly using this apporach.</p>
<ul>
<li><a href="https://arxiv.org/abs/2310.02226" target="_blank" rel="noopener">Think before You Speak</a>. Each inserted sequences of pause tokens rewires the on-going language model to a sampled $\mu_n$.</li>
<li><a href="https://arxiv.org/abs/2310.12143" target="_blank" rel="noopener">Simple Mechanisms for Representing, Indexing and Manipulating Concepts</a> This work associates training samples with subspaces in a model. SPLM associates group of texts with latent clusters.</li>
<li>Another interesting work of Yuanzhi is described in his inspiring talk <a href="https://www.youtube.com/watch?v=M25cbX5do8Y" target="_blank" rel="noopener">Physics of Language Models</a>. He described an augmentation method of permuting sentences in a paragraph can improve the generalization of knowledge in the paragraph into other context. Actually this augmentation can be explained as approximating SPLM, which tries to associate a group text to a cluster of LMs that are aware of the knowledge in this group. So for each sentence $x$, we can expect the LM probability of the entire paragraph should be promoted in the associated cluster of LMs. By permutation, such promotion of probability is essentially applied to $P(|x,M)$ for a language model $M$. Simply put, it is trying to approximate SPLM with a single LM through prompting(or context overloading).</li>
<li>In fact, I studied an almost identical problem and used another method for context overloading in this <a href="https://arxiv.org/pdf/2305.18582" target="_blank" rel="noopener">work</a>, which is also an approximation of SPLM.</li>
</ul>
<h2 id="thoughts-on-implementation">Thoughts on Implementation</h2>
<p>I have some initial thoughts on implementing through variation inference. The key challenge might lies within the potentially large number of latent variables to learn (corresponds to LM paramters), as well as the efficiency concerns since it will requires re-running the previous context when new latent cluster is sampled. I am thinking about implememnting variational inference only on the last few layers. And for efficiency, I am considering using different layers as different mixutres (somewhat inspired by speculative decoding and neural logical models that compose lower level logics in lower layers into higher-order logic in higher layers). I might add more details when I get time.</p>
<p>It might also be approximated through context overloading, as some of existing work is already going towards.</p>

    </div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/academic/">Academic</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://perfec-yu.github.io/post/getting-started/&amp;text=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://perfec-yu.github.io/post/getting-started/&amp;t=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning&amp;body=https://perfec-yu.github.io/post/getting-started/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://perfec-yu.github.io/post/getting-started/&amp;title=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning%20https://perfec-yu.github.io/post/getting-started/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://perfec-yu.github.io/post/getting-started/&amp;title=Thoughts%20on%20Randomizing%20Lanugage%20Modeling%20for%20Better%20Language%20Learning" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://perfec-yu.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu31c85c1b30a921afef60d70fdf4a8a4e_18688_270x270_fill_q75_lanczos_center.jpg" alt="Pengfei Yu"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://perfec-yu.github.io/">Pengfei Yu</a></h5>
      <h6 class="card-subtitle">PhD Student in Computer Science</h6>
      <p class="card-text">My research focuses on information extraction and knowledge learning. I have broader interests on most NLP topics.</p>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d66c8b3b4ad0f66a62428f6bc7cf477d.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>






</body>
</html>
