<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: April 19, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.f6689966c0a10712f95f034011917db0.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Pengfei Yu" />





  

<meta name="description" content="This is a updated version of my previous post on stochastic language models. This post can be read alone, since I have modified the previous idea significantly. Simply put, I still believe a stochastic language model is more desirable than a constant language model, but my previous immature post is subject to a few fallacies." />



<link rel="alternate" hreflang="en-us" href="https://perfec-yu.github.io/post/slm_v2/" />
<link rel="canonical" href="https://perfec-yu.github.io/post/slm_v2/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://perfec-yu.github.io/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_512x512_fill_lanczos_center_3.png" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="于" />
<meta property="og:url" content="https://perfec-yu.github.io/post/slm_v2/" />
<meta property="og:title" content="Updated Thoughts on Randomized Language Models | 于" />
<meta property="og:description" content="This is a updated version of my previous post on stochastic language models. This post can be read alone, since I have modified the previous idea significantly. Simply put, I still believe a stochastic language model is more desirable than a constant language model, but my previous immature post is subject to a few fallacies." /><meta property="og:image" content="https://perfec-yu.github.io/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2024-04-12T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2024-04-12T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://perfec-yu.github.io/post/slm_v2/"
  },
  "headline": "Updated Thoughts on Randomized Language Models",
  
  "datePublished": "2024-04-12T00:00:00Z",
  "dateModified": "2024-04-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Pengfei Yu"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "于",
    "logo": {
      "@type": "ImageObject",
      "url": "https://perfec-yu.github.io/media/icon_hu70abeb15d803708cc6c51bdc94153538_368998_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This is a updated version of my previous post on stochastic language models. This post can be read alone, since I have modified the previous idea significantly. Simply put, I still believe a stochastic language model is more desirable than a constant language model, but my previous immature post is subject to a few fallacies."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Updated Thoughts on Randomized Language Models | 于</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="45426833d7eeb668c104aaea2fdfcdd8" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">于</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">于</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#hobby"><span>Hobbies</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>Updated Thoughts on Randomized Language Models</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Pengfei Yu</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 12, 2024
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/ideas/">Ideas</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="notations">Notations</h2>
<h5 id="conventions-for-notations">Conventions for Notations</h5>
<ul>
<li>Lower case letters for variables</li>
<li>Upper letters for mappings (functions, distributions, etc.)</li>
<li>Calligraphic letters for sets or spaces, except for conventional notations such as $\mathbb{R}$ or $[0,1]$.</li>
<li>Greek letters for random variables</li>
</ul>
<h5 id="list-of-notations">List of Notations</h5>
<ul>
<li>$\mathcal{V}$: the vocabulary, or the set of tokens</li>
<li>$\mathcal{S}=\bigcup_{n=1}^{\infty} \mathcal{V}^n$: the set of token sequences</li>
<li>$\mathcal{P}_\mathcal{S}$: the space of all probability distributions on $\mathcal{S}$ (language models)</li>
<li>$\Pi_\mathcal{S}=\mathcal{P}\circ\mathcal{P}_\mathcal{S}$: the space of distributions over language models</li>
</ul>
<h2 id="the-shorter-version">The Shorter Version</h2>
<p>A language model is optimized to produce a distribution that matches the data distribution. Suppose we have a prior distribution over language models, denoted by $\mu \in \Pi_\mathcal{S}$. We can interpret the learning process as:
$$
M = \argmax_M P(\mathcal{D}|M) P(M|\mu).
$$
I would like to make a strong argument: We should learn $\mu$ instead of any single constant language model $M$. I interpret $\mu$ as a &ldquo;filter&rdquo; that discards all models $H$ where $P(\mathcal{D}|H)=0$. For example, a model that believes both <em>&ldquo;Tom likes playing sports&rdquo;</em> and <em>&ldquo;Tom doesn&rsquo;t like playing sports&rdquo;</em> is not feasible. For any specific query $q$, we should sample from $\mu$ to generate responses:
$$
r \sim P(r|q, \mu) = \int_M P(r|q, M)P(M|\mu) \mathrm{d} M,
$$
and alternatively, we can alter $\mu$ without changing its support set for some &ldquo;unconventional&rdquo; responses (still filtering out impossible models):
$$
r \sim P(r|q, \hat{\mu}) = \int_M P(r|q, M)P(M|\hat{\mu}) \mathrm{d} M.
$$
Here $\hat{\mu} \propto \mu \times g$, and $g:\mathcal{P_\mathcal{S}}\to\mathbb{R}^+$.</p>
<p>A stochastic language model should, at least, consist of</p>
<ul>
<li>a learned prior from data over language models $\mu$</li>
<li>a modifying function $g:\mathcal{P_\mathcal{S}}\to\mathbb{R}^+$ that steers behaviors of $\mu$ towards a more desired direction</li>
</ul>
<p>$\mu$ characterizes all possible hypothesis potentially by assuming rules or facts absent in the training data. In comparison, a constant language model is just one hypothesis within $\mu$ that is considered most &ldquo;likely&rdquo; given the observed data, which could be biased or inaccurate.</p>
<h3 id="thoughts-on-implementation">Thoughts on Implementation</h3>
<p>In a discrete formulation of component language models (similar to mixture-of-experts), let&rsquo;s assume we have $K$ components $M_{1:K}$, each of them assumes a unique set of facts $\mathcal{A}_{1:K}$. We can model each component as a base model $B$ plus additional assumptions:
$$
P(\cdot|M_k) = P(\cdot|B, \mathcal{A}_k).
$$
To implement this modeling, I am thinking of two potential approaches.</p>
<ul>
<li>
<p><em><strong>In-context Components</strong></em>: Formulate each $\mathcal{A}_{k}$ as a set of explicitly or implicitly context prepended to the model. These contexts should be sampled and optimized during the pre-training stage.</p>
</li>
<li>
<p><em><strong>Layer Selection</strong></em> Consider each set of assumptions as a unique combination of model layers. The model dynamically sample a combination of the layers to reflect different components.</p>
</li>
</ul>
<h2 id="the-longer-version-with-rationales">The Longer Version with Rationales</h2>
<h3 id="constant-language-models">Constant Language Models</h3>
<p>A language model $M\in\mathcal{P}<em>\mathcal{S}$ is a distribution over token sequences: It gives a probability $P(s|M)$ for any sequence $s\in\mathcal{S}$. Moreover, given any partial sequence $x</em>{1:n}\in \mathcal{V}^n$ from a longer sequence $x_{1:m}\in\mathcal{V}^m$, the language model naturally computes a conditional distribution $P(x_{n+1:m}|x_{1:n}, M)=\frac{P(x_{1:m}|M)}{P(x_{1:n}|M)}$. This is used by &ldquo;Chat&rdquo; models, which response to a user query $q$ by sampling a response $r\sim P(\cdot|q, M)$.</p>
<p>We usually train such a model over three stages:</p>
<ul>
<li>Pre-train on a text corpus</li>
<li>Supervised fine-tuning (SFT) on query-response pairs</li>
<li>Preference alignment on human preference data of (query, preferred response, disliked response) triples</li>
</ul>
<p>In all three stages, a language model is optimized to produce a distribution that matches the data distribution. Suppose we have a prior distribution over language models, denoted by $\mu \in \Pi_\mathcal{S}$. The prior comes from the architectural design and the modeling of the language (e.g., autoregressive transformers). We can interpret the learning process as the Bayesian inference:
$$
M = \argmax_M P(\mathcal{D}|M) P(M|\mu).
$$</p>
<p>It might seem tricky to interpret the preference alignment in a Bayesian fashion. However, since RLHF and its variants are essentially optimizing a text distribution where responses&rsquo; observed frequencies are rescaled according to the rewards assigned, the preference alignment also fits the framework.</p>
<p>Despite the success of language modeling demonstrated by models like GPT-4, I think this paradigm is fundamentally undesirable because <strong><em>Language Modeling Only Explains Data In a Most Probable Way</em></strong>.</p>
<p>First consider a uniform prior assignment $\mu$, where the optimization becomes
$$
M = \argmax_M P(\mathcal{D}|M).
$$</p>
<p>This means that the optimal language model under uniform prior gives zero probability to unseen or novel sentences, indicating zero generalization. No matter how much data we have, this is not desired since:</p>
<ul>
<li><strong><em>No generalization means no new knowledge can be created.</em></strong> For example, the learned model cannot help researchers on innovative ideas, or solving problems that are not yet solved by humans.</li>
<li><strong><em>Mimicking the real world is not always the best strategy</em></strong>. For example, if the training data contains toxic or biased content, the learned model will be exactly toxic or biased to the same extent. Another toy example would be predicting results of tossing an imbalanced coin (say $0.6$ head and $0.4$ tail). A desired model that maximize accuracy should always predict &ldquo;head&rdquo;, but a model that matches observations only predicts head with a probability of $0.6$.</li>
</ul>
<p>Hence, if we stick to the optimization objective above, the prior $\mu$ must be non-trivial to play a role in achieving generalization. Fortunately, current LLMs do have a non-trivial prior, since they have been remarkable in generalizing to unseen data. However, what are these priors?</p>
<ul>
<li>It is hard to figure out the architectural prior of autoregressive transformers. There is an intuition-based qualitative characterization of this prior though: sentences that look similar have correlated probabilities. This prior makes sense sometimes. For example, <em>&ldquo;Tom likes playing sports&rdquo;</em> is somewhat correlated to <em>&ldquo;Tom enjoys playing sports&rdquo;</em>. However, we also observe weird correlations: <em>&ldquo;Tom likes playing sports&rdquo;</em> could also correlate positively to <em>&ldquo;Tom doesn&rsquo;t like playing sports&rdquo;</em>, meaning that increasing one probability leads to an increase in the other.</li>
<li>In the stage-wise training paradigm, a checkpoint from an earlier stage naturally carries a prior from the previous data. For example, SFT is essentially optimizing a model with a prior dependent on the pre-training data. Moreover, we don&rsquo;t always pre-train models until convergence. This means that earlier samples may form a prior to later samples, which indicates that the order of samples during training could be important.</li>
</ul>
<p>I suppose the existing prior for LLMs is not perfect. In fact, recent work on knowledge editing shows that generalizing an edited knowledge is very hard. Moreover, the well-known reversal curse also indicates that the current prior from the autoregressive language modeling does not generalize well enough. I think a perfect prior $\mu$ should potentially encompass all the rules for generalization such as logical deduction rules. Moreover, these rules should be learned from data, either through explicit mentions of rules or implicit induction from observations, rather than prepared by human in advance, since it is hard to exhaust all such rules. <strong><em>$\mu$ should come from the data.</em></strong></p>
<p>I interpret $\mu$ as a &ldquo;filter&rdquo; that discards all models $H$ where $P(\mathcal{D}|H)=0$ (or smaller than a threshold), reflecting rules of deduction. For example, a model that believes both <em>&ldquo;Tom likes playing sports&rdquo;</em> and <em>&ldquo;Tom doesn&rsquo;t like playing sports&rdquo;</em> is not a feasible model and should be discarded. In other words, $\mu$ characterizes all possible hypothesis that can explain the real world, potentially by assuming unstated facts absent in the training data.</p>
<!-- One might argue that a pretrained model naturally learns such prior gradually, but this is not necessarily true.  -->
<!-- Moreover, if the training data contains toxic or biased content, the model should be exactly toxic or biased to the same extent.

Fortunately, models like GPT-4 does generalize to unseen sequences.  -->
<!-- However, such a constant language model to me is like a "dead" model that doesn't accept any more changes. BTW, from my understanding, this is also the reason why it is hard to even formulate an objective for continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF. The cross-entropy loss is not the real objective, since convergence to this loss on the fine-tuning data would result in catastrophic forgetting. In most cases we can just define some heuristic criteria to stop the fine-tuning. Adding additional regularization terms such as $\lambda \|\|\theta-\theta_0\|\|_2^2$ to the cross entropy might serve as a valid objective, while acute readers might already see this is close to considering language model as a "Gaussian language model" centered at the pretrained state.  -->
<h3 id="stochastic-language-models">Stochastic Language Models</h3>
<p>I would like to make a strong argument about learning language models: We should learn $\mu$ instead of any single constant language models. For any specific query $q$, we should sample from $\mu$ to generate responses:
$$
r \sim P(r|q, \mu) = \int_M P(r|q, M)P(M|\mu) \mathrm{d} M,
$$
and alternatively, we can alter $\mu$ without changing its support set for some &ldquo;unconventional&rdquo; responses (still filtering out impossible models):
$$
r \sim P(r|q, \hat{\mu}) = \int_M P(r|q, M)P(M|\hat{\mu}) \mathrm{d} M.
$$
Here $\hat{\mu} \propto \mu \times g$, and $g:\mathcal{P_\mathcal{S}}\to\mathbb{R}^+$.</p>
<p>A stochastic language model should, at least, consist of</p>
<ul>
<li>a learned prior from data over language models $\mu$</li>
<li>a modifying function $g:\mathcal{P_\mathcal{S}}\to\mathbb{R}^+$ that steers behaviors of $\mu$ towards a more desired direction</li>
</ul>
<p>This formulation mimics the current paradigm in that: (1) Learning $\mu$ corresponds to pre-training. (2) Learning $g$ corresponds to preference alignment, except that we pick preferred language models instead of altering language model distribution.</p>
<p>I also want to further compare the learning of $\mu$ with the learning of a constant language model.</p>
<ul>
<li>A constant language model is the best probabilistic explanation of the data given some prior. However, it is not guaranteed to be correct, and in fact, there is no guarantee that we can find the correct model of the world from observations we have: Otherwise there is no more need for scientific research to understand the world. However, the &ldquo;best&rdquo; model refuses other plausible hypothesis. It should be questioned whether it is really optimal, since data could be of insufficient accuracy: Thinking of Newton&rsquo;s Laws versus Theory of Relativity, the former could be optimal or at least equally optimal when we only observe data in low-speed scenarios.</li>
<li>Fine-tuning a constant language model (or RLHF) sometimes involves a KL-regularization term. We can interpret this procedure as treating a constant language model as a Gaussian stochastic language model centered around its pre-trained state. However, this interpretation implicitly implies that we should not deviate too much from a pre-trained language model, or all modifications can be &ldquo;local&rdquo;. However, in real world, there are cases where we have two contradicting hypotheses, and upon subsequent observations we choose one of them. We may need drastic changes to correct errors. This is not a problem for alignment of $\mu$.</li>
</ul>
<!-- Another two similar things that one might think of are mixture of experts or VAE. Firstly for mixture of experts which typically works on finite components, the space of language models might not only be too large, but also uncountable (meaning it cannot be indexed by natural numbers in case someone may be unfamiliar with this concept. Set of even numbers is countable but set of real numbers is not.). VAE is indeed more related. In fact I think adpating the ELBO from VAE is one possible way of learning such a model. And apart from the apparently much larger latent space than VAEs we used to see in practice, there is something else fundamentally different as I will show after explaining the stochastic process of generation. -->
<!-- For stochastic process of generation, it can simply be understood as a sequence $(\xi_i,\mu_i), i\in[1,n]$.  Here each $\xi_i$ corresponds to the random variable of a generated token, and $\mu_i$ is the i-th step language model. In this process, a stochastic language model $\mu$ essentially serve as some sort of prior. From the theory of VAE $\mu_i$ can be encoded using the entire sequence for learning. But for now we can simply ignore everything about learning and simply formulate the generation process as -->
<!-- $$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)\hat{P}(\mu_i|\xi_{1:i-1}, \mu).
$$
There are actually a few alternative ways of decomposing this probabilities and this is just one of them. I choose this one because it aligns best with my understanding of how human compose sentences. I would like to add a few notes here before proceeding.
- I added a hat in $\hat{P}(\mu_i|\xi_{1:i-1}, \mu)$. Mathematically, $P(\mu_i|\xi_{1:i-1}, \mu)$ can be computed as a posterior once the prior $\mu$ is learned. However, I think this posterior is only a model for the observed language, not for the generation that serves certain purpose or fulfills certain tasks. This is also the key difference between this process and VAE. Human are not parrots that replicates the distribution it oberserves, but rather produce contents consistent with themselves' values, beliefs, habits, knowledge and personalities etc. I will discuss how to get $\hat{P}$ later.
- $P(\xi_i|\xi_{1:i=1}, \mu_i)$ discards the beautiful markov property and requires to re-compute everything when switching to a new language model. This might looks redundant at first glance but it will be explained.
- For learning, we already see that $\mu$, the prior, needs also be learned rather than taking standard Gaussian as the original VAE. For example, we can take $\mu$ as a mixture of Gaussian and use a two-level EM (outer level for MoG and inner level for VAE).

Since now $\hat{P}(\mu_i|\xi_{1:i-1}, \mu)$ is not the probability of $P(\mu_i|\xi_{1:i-1}, \mu)$, we shall consider it to be generated by another model (encoder in VAE) and rewrite $P(\mu_i|\xi_{1:i-1}, \mu, G)$ where $G$ is the new model. Now, a SPLM is simply defined as a pair of $(\mu, G)$, If we look closer at what $G$ is doing, it associates some text $\xi_{1:i-1}$ with a stochastic language model. From the derivation of VAE (or EM, KMeans or just intuition, whichever is best for you), $G$ is, very probably, grouping a set of text $\mathcal{T}$ together into a conditional stochastic language model $\mu_\mathcal{T}$. One possible interpretation of $\mu_\mathcal{T}$ would be **_a stochastic language model that is aware of the knowledge contained in $\mathcal{T}$_**. This is already one desired property that is not so clear in LLMs: we can **_pinpoint knowledge_** of a model into a latent stochastic LM, and moreover, the knowledge we are using at each step during generation can be interpreted.

Now, I will simply incorporte the constant language model into this framework as the last step of this section. Let $M=\mathbb{E}\mu$ (treat $\mu$ as an infinite-dimensional random vector and expectation is taken element-wise), and $G$ is a function that always produces a delta-distribution (one-hot) at $M$. In other words, $M$ is simply the expectation of the stochastic language model. What is bad about expectation? The most straightforward outcome is **_hallucination_**. It can be better illustrated with a non-linguistic example. Consider a ball falls uniformly on $x=-1$ or $x=1$, the expected position would be $x=0$ where the ball will never falls on. Similarly, a SPLM might pick either $\phi$ or $\psi$ for the next generation, but never an average of them. One last comment I would like to leave is that, since we observe that LM is just a first-order approximation of the SPLM, adding any higher-order momentums or information about their joint distributions would help. If you simply expand the higher-order moments with infinite-dimensional random vector view of the distributions, higher-order momentums can been seen as modeling some multi-concept relations or multiary logical rules. We can do further analysis on this process or add additional assumptions but I think it is good enough to stop here and show some concrete examples. But before that, I will summarize the math content in case you lost somewhere in the middle. -->
<!-- ### Summary of Mathematics
A SPLM is a pair of $(\mu, G)$. $\mu$ is some prior distribution of stochastic language models. Intuitively, it can contain several clusters. $G$ is a function that associates a group of text with another cluster of language models that are aware of the knowledge contained in the group of text, while this association can depend on some values or preferences we set to the model. For language modeling (evaluation of probabilities), 
$$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)P(\mu_i|\xi_{1:i-1}, \mu),
$$
$G$ is ignored since the values we set doesn't reflect the natural language distribution. $P(\mu_i|\xi_{1:i-1}, \mu)$ can be estimated through Bayesian inference. For generation,
$$
P(\xi_{1:n}) = P(\xi_1|\mu_1)P(\mu_1|\mu) \prod_{i=2}^{n} P(\xi_i|\xi_{1:i=1}, \mu_i)P(\mu_i|\xi_{1:i-1}, G, \mu).
$$
## SPLM and Challenges of LLMs

We already see how SPLM supports better knowledge localization and reduced hallucination from the math part. I will list some other topics that people around or me are working on. Everyone is welcome to contribute to this list. For the following sections, I will explain more on some topics I am more familiar with and less on those I only have basic knowledge or are overly simple.

### Modeling Deterministic Rules/Bias
From my perspective, the cause of failure in LLMs in these two topics are essentially the same as hallucination: the averaging effect of taking expectation. In SPLM, deterministic rules might be modeled into a separate language model where there are a lot of $P(x|y)=1$ and it is simply a latent component of $\mu$. The function $G$ will call this component when we need to do deterministic reasoning. Similarly, we can set preferences in $G$ between the biased and unbiased components for debiasing. One might think that, since we want an unbiased model, isn't it equivalent to directly learn the induced LM from a $G,\mu$ when $G$ is set with the desired preference? There are several considerations here. Firstly, LLMs doesn't explicitly model the latent components, so training the LLM to imitate an unbiased distribution may destruct other knowledge or the logical consistency of the LLM. To be more straightforward consider $A$ is a biased opinion and $B$ refers to biased people. $P(A)=P(A|B)P(B)+P(A|\neg B)P(\neg B)$. If we simply train the model to have $P(A)=0$, the LLM could, unrigorsly speaking,
- forms the belief that biased people are no longer biased $P(A|B)=0$;
- forms the belief that is logically inconsistent (breaking the above equality): even though there is biased speech from biased people, there is no biased speech.

I personally believe the second cases should happen more frequently. In fact this is somewhat validated by the observed ripple effect in the next part for knowledge editing.

### Ripple Effect in Knowledge Editing and Fine-tuning

Ripple effect refers to the effect of editing one fact might affect other facts or create new facts. For example,
- Fact: Messi has won a total of 7 Ballon d’Or b by 2022.
- Knowledge Edit: Messi won Ballon d'Or for 2023
- New Fact: Messi has won a total of 8 Ballon d'Or by 2023.

I personally think this is not only a problem for editing or fine-tuning, but for training (including pre-training) as a whole. Language models, even GPT-4, display weak ripple effect throughout the learning process. I speculate this to originates from the failure of modeling higher-order features of language models being a first order approximation of stochastic language models, which is related to the joint distribution of $P(P(x|\mu), P(y|\mu))$ for related (or even unrelated) facts $x,y$. Note that since $\mu$ is a random variable, $P(x|\mu)$ is also a random variable. So the above distribution is also a "distribution of distribution". Further analysis would require more math so I will just stop here for now. From an intuitive perspective, training SPLM for new knowledge involves encoding new knowledge to corresponding clusters that represent a group of text. So the effect naturally propagates to relevant facts. Below are two more complex examples of ripple effect in GPT-4. The left column show some knowledge it knows and right column shows that it cannot use the knowledge for other tasks.




















<figure  id="figure-ripple-effect-example-of-gpt4-1">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="ripple_1.png" alt="Ripple Effect Example of GPT4 (1)" loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Ripple Effect Example of GPT4 (1)
    </figcaption></figure>



















<figure  id="figure-ripple-effect-example-of-gpt4-2">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="ripple_2.png" alt="Ripple Effect Example of GPT4 (2)" loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Ripple Effect Example of GPT4 (2)
    </figcaption></figure>


### Formulating Continual Fine-tuning

Actually I am not knowledgeable enough about the continual fine-tuning of EM algorithms, such as whether it can also be formulated as the converging point under certain algorithms. But at least, I think new examples can first be encoded into the space of stochastic language models, and if they appears to be a new cluster that is far away from existing clusters, we need to assign a new component for them.

### Multimodality

Note that the encoding of a sample refers to mapping a sample into a latent cluster of stochastic language models. For a multimodal model, this provides a more direct way of examining related concepts in various modalities (mapping to the same cluster).

### Mappings Among Models
In SPLMs, we only need to map latent clusters to align two different models. Such alignment can be achived by using a set of sentences as test bed to see the probabilities of corresponding clusters on two models given each context.

## Augmentation Methods and Recent Work that Can be Explained with SPLM

### Tool learning and Retrieval Augmentation

Both of these two popular methods have the following format: for a sequence splitted into two parts, $x_{1:m};x_{m+1:n}$, a function produces some additional information from $F(z|x_{1:m})$, and the model generates the rest part with $P(x_{m+1:n}|x_{1:m},z)$. There are two ways of understanding this framework. 
- $G=F$ and $P(|z)$ is a sampled $\mu_n$;
- $F$ is simply a sampled latent stochastic language model
There is no essential difference between this two, because the second one is equivalent to setting certain preference to $G$ under certain context. Under the first understanding, we can simply fine-tune $G$ (which i assume to be smaller since it is only a planner with limited knowledge). Under the second, it is actually more interesting. Since tools or retrievers can be considered as functions, we can simply get a set of input output pairs, $(x,y)$ and see if any existing cluster in $\mu$ already models this behavior. If not, we can train to incorporate a new cluster (function). Furthermore, if we firmly believe that a tool is more accurate than trained models (e.g., calculator), we can always call the tool when $G$ produces the new cluster. In essence, we are able to map almost any functions into the parameter space of the $\mu$. 
PS: Look at this [paper](https://arxiv.org/abs/2104.09841) for retrieval augmentation. I already remember a simiar tool learning paper like this but cannot retrieve the name from my memory lol.

### Role Playing and Other Smart Prompting (CoT, In-context Learning)

Simply put, $P(|z)$ is a sampled $\mu_n$ for the prompt $z$, no matter inserted at the beginning or in the middle. A little more notes for in-context learning: it can be considered as forming a new cluster with the knowledge of provided examples.

### Alignment Methods (SFT & RLHF) -->
<!-- Corresponds to fine-tuning $G$. However, in SPLM training $G$ will definitely not affect the knowledge structure (similar to the example explained in the debiasing), since knowledge is stored in $\mu$. -->
<!-- ### Some Recent Work

This is definitely an incomplete list of work that is related to this framework. I just kind of randomly picked a few since this post is already too long. Also since I already relates this framework with many approaches, I am only showing those work that are not directly using this apporach.
- [Think before You Speak](https://arxiv.org/abs/2310.02226). Each inserted sequences of pause tokens rewires the on-going language model to a sampled $\mu_n$.
- [Simple Mechanisms for Representing, Indexing and Manipulating Concepts](https://arxiv.org/abs/2310.12143) This work associates training samples with subspaces in a model. SPLM associates group of texts with latent clusters. 
- Another interesting work of Yuanzhi is described in his inspiring talk [Physics of Language Models](https://www.youtube.com/watch?v=M25cbX5do8Y). He described an augmentation method of permuting sentences in a paragraph can improve the generalization of knowledge in the paragraph into other context. Actually this augmentation can be explained as approximating SPLM, which tries to associate a group text to a cluster of LMs that are aware of the knowledge in this group. So for each sentence $x$, we can expect the LM probability of the entire paragraph should be promoted in the associated cluster of LMs. By permutation, such promotion of probability is essentially applied to $P(|x,M)$ for a language model $M$. Simply put, it is trying to approximate SPLM with a single LM through prompting(or context overloading). 
- In fact, I studied an almost identical problem and used another method for context overloading in this [work](https://arxiv.org/pdf/2305.18582), which is also an approximation of SPLM. -->
<h3 id="thoughts-on-implementation-1">Thoughts on Implementation</h3>
<p>Firstly, different components in $\mu$ are based on different assumptions absent in the training data. In a discrete formulation of component language models (similar to mixture-of-experts), let&rsquo;s assume we have $K$ components $M_{1:K}$, each of them assumes a unique set of facts $\mathcal{A}_{1:K}$. We can model each component as a base model $B$ plus additional assumptions:
$$
P(\cdot|M_k) = P(\cdot|B, \mathcal{A}_k).
$$
To implement this modeling, I am thinking of two potential approaches.</p>
<h5 id="in-context-components">In-context Components</h5>
<p>We may formulate each $\mathcal{A}_{k}$ as a set of explicitly or implicitly context prepended to the model. These contexts should be sampled and optimized during the pre-training stage. I don&rsquo;t have a clear idea of how to make this efficient yet. Moreover, this paradigm inevitably increase the inference cost.</p>
<h5 id="layer-selection">Layer Selection</h5>
<p>We may consider each set of assumptions as a unique combination of model layers. During forward passes, the model dynamically sample a combination of the layers to reflect different components. This should be efficient, and can be optimized via variational inference.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/academic/">Academic</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F&amp;text=Updated&#43;Thoughts&#43;on&#43;Randomized&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F&amp;t=Updated&#43;Thoughts&#43;on&#43;Randomized&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Updated%20Thoughts%20on%20Randomized%20Language%20Models&amp;body=https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F&amp;title=Updated&#43;Thoughts&#43;on&#43;Randomized&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Updated&#43;Thoughts&#43;on&#43;Randomized&#43;Language&#43;Models%20https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fperfec-yu.github.io%2Fpost%2Fslm_v2%2F&amp;title=Updated&#43;Thoughts&#43;on&#43;Randomized&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://perfec-yu.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu31c85c1b30a921afef60d70fdf4a8a4e_18688_270x270_fill_q75_lanczos_center.jpg" alt="Pengfei Yu"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://perfec-yu.github.io/">Pengfei Yu</a></h5>
      <h6 class="card-subtitle">PhD Student in Computer Science</h6>
      <p class="card-text">My research focuses on information extraction and knowledge learning. I have broader interests on most NLP topics.</p>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.ee7be7b7b2cf2cd4bb899879abb5b47b.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>


















</body>
</html>
