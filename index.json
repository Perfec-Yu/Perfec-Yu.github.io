
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"My Chinese name is 于(Yu)鹏(Peng)飞(fei). I am a PhD student in computer science at UIUC blender lab advised by Heng Ji. My research interests include information extraction (IE) and its applications in NLP. I also have some experience on a variety of topics, such as meta learning, transfer learning, continual learning, weakly-supervised learning and multimodal learning.\n","date":1712880000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1712880000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My Chinese name is 于(Yu)鹏(Peng)飞(fei). I am a PhD student in computer science at UIUC blender lab advised by Heng Ji. My research interests include information extraction (IE) and its applications in NLP.","tags":null,"title":"Pengfei Yu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://perfec-yu.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Pengfei Yu"],"categories":["Ideas"],"content":"Notations Conventions for Notations Lower case letters for variables Upper letters for mappings (functions, distributions, etc.) Calligraphic letters for sets or spaces, except for conventional notations such as $\\mathbb{R}$ or $[0,1]$. Greek letters for random variables List of Notations $\\mathcal{V}$: the vocabulary, or the set of tokens $\\mathcal{S}=\\bigcup_{n=1}^{\\infty} \\mathcal{V}^n$: the set of token sequences $\\mathcal{P}_\\mathcal{S}$: the space of all probability distributions on $\\mathcal{S}$ (language models) $\\Pi_\\mathcal{S}=\\mathcal{P}\\circ\\mathcal{P}_\\mathcal{S}$: the space of distributions over language models The Shorter Version A language model is optimized to produce a distribution that matches the data distribution. Suppose we have a prior distribution over language models, denoted by $\\mu \\in \\Pi_\\mathcal{S}$. We can interpret the learning process as: $$ M = \\argmax_M P(\\mathcal{D}|M) P(M|\\mu). $$ I would like to make a strong argument: We should learn $\\mu$ instead of any single constant language model $M$. I interpret $\\mu$ as a “filter” that discards all models $H$ where $P(\\mathcal{D}|H)=0$. For example, a model that believes both “Tom likes playing sports” and “Tom doesn’t like playing sports” is not feasible. For any specific query $q$, we should sample from $\\mu$ to generate responses: $$ r \\sim P(r|q, \\mu) = \\int_M P(r|q, M)P(M|\\mu) \\mathrm{d} M, $$ and alternatively, we can alter $\\mu$ without changing its support set for some “unconventional” responses (still filtering out impossible models): $$ r \\sim P(r|q, \\hat{\\mu}) = \\int_M P(r|q, M)P(M|\\hat{\\mu}) \\mathrm{d} M. $$ Here $\\hat{\\mu} \\propto \\mu \\times g$, and $g:\\mathcal{P_\\mathcal{S}}\\to\\mathbb{R}^+$.\nA stochastic language model should, at least, consist of\na learned prior from data over language models $\\mu$ a modifying function $g:\\mathcal{P_\\mathcal{S}}\\to\\mathbb{R}^+$ that steers behaviors of $\\mu$ towards a more desired direction $\\mu$ characterizes all possible hypothesis potentially by assuming rules or facts absent in the training data. In comparison, a constant language model is just one hypothesis within $\\mu$ that is considered most “likely” given the observed data, which could be biased or inaccurate.\nThoughts on Implementation In a discrete formulation of component language models (similar to mixture-of-experts), let’s assume we have $K$ components $M_{1:K}$, each of them assumes a unique set of facts $\\mathcal{A}_{1:K}$. We can model each component as a base model $B$ plus additional assumptions: $$ P(\\cdot|M_k) = P(\\cdot|B, \\mathcal{A}_k). $$ To implement this modeling, I am thinking of two potential approaches.\nIn-context Components: Formulate each $\\mathcal{A}_{k}$ as a set of explicitly or implicitly context prepended to the model. These contexts should be sampled and optimized during the pre-training stage.\nLayer Selection Consider each set of assumptions as a unique combination of model layers. The model dynamically sample a combination of the layers to reflect different components.\nThe Longer Version with Rationales Constant Language Models A language model $M\\in\\mathcal{P}\\mathcal{S}$ is a distribution over token sequences: It gives a probability $P(s|M)$ for any sequence $s\\in\\mathcal{S}$. Moreover, given any partial sequence $x{1:n}\\in \\mathcal{V}^n$ from a longer sequence $x_{1:m}\\in\\mathcal{V}^m$, the language model naturally computes a conditional distribution $P(x_{n+1:m}|x_{1:n}, M)=\\frac{P(x_{1:m}|M)}{P(x_{1:n}|M)}$. This is used by “Chat” models, which response to a user query $q$ by sampling a response $r\\sim P(\\cdot|q, M)$.\nWe usually train such a model over three stages:\nPre-train on a text corpus Supervised fine-tuning (SFT) on query-response pairs Preference alignment on human preference data of (query, preferred response, disliked response) triples In all three stages, a language model is optimized to produce a distribution that matches the data distribution. Suppose we have a prior distribution over language models, denoted by $\\mu \\in \\Pi_\\mathcal{S}$. The prior comes from the architectural design and the modeling of the language (e.g., autoregressive transformers). We can interpret the learning process as the Bayesian inference: $$ M = \\argmax_M P(\\mathcal{D}|M) P(M|\\mu). $$\nIt might seem tricky to interpret the preference alignment in a Bayesian fashion. However, since RLHF and its variants are essentially optimizing a text distribution where responses’ observed frequencies are rescaled according to the rewards assigned, the preference alignment also fits the framework.\nDespite the success of language modeling demonstrated by models like GPT-4, I think this paradigm is fundamentally undesirable because Language Modeling Only Explains Data In a Most Probable Way.\nFirst consider a uniform prior assignment $\\mu$, where the optimization becomes $$ M = \\argmax_M P(\\mathcal{D}|M). $$\nThis means that the optimal language model under uniform prior gives zero probability to unseen or novel …","date":1712880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712880000,"objectID":"45426833d7eeb668c104aaea2fdfcdd8","permalink":"https://perfec-yu.github.io/post/slm_v2/","publishdate":"2024-04-12T00:00:00Z","relpermalink":"/post/slm_v2/","section":"post","summary":"This is a updated version of my previous post on stochastic language models. This post can be read alone, since I have modified the previous idea significantly. Simply put, I still believe a stochastic language model is more desirable than a constant language model, but my previous immature post is subject to a few fallacies.","tags":["Academic"],"title":"Updated Thoughts on Randomized Language Models","type":"post"},{"authors":["Pengfei Yu","Jiateng Liu, Pengfei Yu,","Yuji Zhang","Sha Li","Zixuan Zhang","Heng Ji"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1708128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708128000,"objectID":"66591d6df141a468f01074265ecdc155","permalink":"https://perfec-yu.github.io/publication/liu-etal-2024-evedit/","publishdate":"2024-02-17T00:00:00Z","relpermalink":"/publication/liu-etal-2024-evedit/","section":"publication","summary":"Counterfactual triple-based knowledge editing is subject to logical fallacies due to a lack of deduction anchors. We fix it by introducing EVent-based Editing.","tags":["Source Themes"],"title":"EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries","type":"publication"},{"authors":["Pengfei Yu"],"categories":["Ideas"],"content":"What This Is About TLDR: This is about building a new framework that is not based on a constant language model, but a stochastic language model. This new framework explains a lot of challenges and recent work on LLMs and provides insights on improving language learning. Then click there to the main content\nI recently came up with notion of modeling language not as a constant language model, but a random variable or a stochastic language model. This idea originates from many things I have worked on, I am working on, and I have read about. Also, it is not a completely new idea, instead it is related, similar to, and combines several established methods and mathematical models, though with minor differences to each of them. In fact, I wouldn’t even expect any framework that completely negates everything intelligent researchers have built for AI and machine learning. Anyway, I have been trying to formulate this stochastic language model and see what kind of insights we can get from this perspective. Interestingly, as I am developing this framework, I notice this is closely related a lot of challenges or methodologies in current LLM research, including but not limited to hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation, mixture-of-experts and multi-modality (the connection between randomizing a language model and multi-modality sounds a little weird, but I will explain later).\nCurrently, I have sort out the most basic concepts of what it means to randomize a language model, what are its implications benefits and roughly how we should build this model or improve existing LLMs by approximating this model. However, I found that it is not trivial to explain this idea to others in a short, e.g., 30min to 1h, period of time and using more brief media of communications such as slides. I was only able to convey and improve most of my thoughts through multiple conversations and active discussions with some of my colleague PhD students. However, due to the level of generality as it seems to me for now, I think this notion of stochastic language modeling is kind of useful for research in LLMs. Moreover, limited by my personal mathematical capability and available computational and data resources, it seems very challenging to fully implement everything I am thinking of under this framework. Also, I think some ongoing research in the community could be closely related to this framework. So I think maybe it is simply better to write about this idea for broader discussions, suggestions and potential collaboration to further improve this idea and bring this idea to reality. Another good thing about a post is that it doesn’t have to like research papers or a set of slides that need to follow certain structures. I feel more comfortable talking about my thoughts in this style.\nIn this post, I will try my best, although it might be kind of hard, to present this idea in an organized way. It might happen that you get a bit lost at the start, but if you keep reading, it is very possible at some point that at some point you realize “oh this is related to/ kind of explains what I am working on / have worked on”. If you are interested in further discussion on this idea, please reach out at pengfei4 at illinois dot edu.\nOverview I will try to explain this idea in the following aspects.\nBasic mathematical definition of the stochastic language modeling. I shall use the name Stochastic Procedural Language Modeling or SPLM since it is better to explain as a stochastic process rather than a single random variable. I know it could be elusive and boring to start with math rather than vivid examples, but I found that even a basic notion of what we are aiming for will benefit the understanding the “vivid examples”. The math part will not go beyond undergraduate-level probability theory, since I will not go into deeper analysis of the stochastic process. The purpose is just to leave a basic impression of the objective. How this framework gives better explanation of various challenges mentioned above in LLMs, and how it is potentially easier to solve such challenges under this framework. Explaining that a series of augmentation methods proposed for LLM are trying to approximate this framework. Moreover, how this framework explains an incomplete collection recent work. Some initial thoughts on how to implement this idea, and how flexible this implemented framework will be in representing not only language but many intelligent behaviors. (to-be finished) Definition of Stochastic Procedural Language Model Conventions for Notations Lower case letters for variables Upper letters for mappings (functions, distributions, etc.) Calligraphic letters for sets or spaces, except for conventional notations such as $\\mathbb{R}$ or $[0,1]$. Greek letters for random variables List of Notations $\\mathcal{V}$: vocabulary, or set of tokens $\\mathcal{S}=\\bigcup_{n=1}^{\\infty} …","date":1698192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698192000,"objectID":"4b9d0265a52749f3a7867809454c46d5","permalink":"https://perfec-yu.github.io/post/splm/","publishdate":"2023-10-25T00:00:00Z","relpermalink":"/post/splm/","section":"post","summary":"Considering language models as random variables brings insights into the challenges in LLMs and the mechanisms of various augmentation methods to improve LLMs on hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation...","tags":["Academic"],"title":"Thoughts on Randomizing Lanugage Modeling for Better Language Learning","type":"post"},{"authors":["Pengfei Yu","Heng Ji"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1685318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685318400,"objectID":"5af0824c5ba95294cc78e302ac232d29","permalink":"https://perfec-yu.github.io/publication/yu-etal-2023-information/","publishdate":"2023-05-29T00:00:00Z","relpermalink":"/publication/yu-etal-2023-information/","section":"publication","summary":"Language modeling doesn't imply information modeling. Optimzing LM probabilities is not enough to keep LLMs updated.","tags":["Source Themes"],"title":"Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy","type":"publication"},{"authors":["Pengfei Yu","Zixuan Zhang","Clare Voss","Jonathan May","Heng Ji"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"3d89774d919c16ace6b79dc80a4f160f","permalink":"https://perfec-yu.github.io/publication/yu-etal-2022-event/","publishdate":"2022-06-25T20:43:20.041226Z","relpermalink":"/publication/yu-etal-2022-event/","section":"publication","summary":"","tags":null,"title":"Event Extractor with Only a Few Examples","type":"publication"},{"authors":["Xinya Du","Zixuan Zhang","Sha Li","Pengfei Yu","Hongwei Wang","Tuan Manh","Xudong Lin","Ziqi Wang","Iris Liu","Ben Zhou","Haoyang Wen","Manling Li","Darryl Hannan","Qi Zeng","Qing Lyu","Charles Yu","Carl Edwards","Xiaomeng Jin","Yizhu Jiao","Ghazaleh Kazeminejad","Rotem Dror","Zhenhailong Wang","Chris Callison-Burch","Mohit Bansal","Carl Vondrick","Jiawei Han","Dan Roth","Shih-Fu Chang","Martha Palmer","Heng Ji"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b5cd5af43a60aaa8de259aa3e4eaff35","permalink":"https://perfec-yu.github.io/publication/du-etal-2022-resin/","publishdate":"2022-06-25T20:43:20.042461Z","relpermalink":"/publication/du-etal-2022-resin/","section":"publication","summary":"","tags":null,"title":"RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios","type":"publication"},{"authors":["Manling Li","Revanth Gangi Reddy","Ziqi Wang","Yi-shyuan Chiang","Tuan Lai","Pengfei Yu","Zixuan Zhang","Heng Ji"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"fe60d84adb5c84109cda7c7a2d97a8a7","permalink":"https://perfec-yu.github.io/publication/li-etal-2022-covid/","publishdate":"2022-06-25T20:43:20.043176Z","relpermalink":"/publication/li-etal-2022-covid/","section":"publication","summary":"To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation.","tags":null,"title":"COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System","type":"publication"},{"authors":["Pengfei Yu","Heng Ji","Prem Natarajan"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f2c54e1633764b9a72306aa1cf5d88f8","permalink":"https://perfec-yu.github.io/publication/yu-etal-2021-lifelong/","publishdate":"2022-06-25T20:43:20.043856Z","relpermalink":"/publication/yu-etal-2021-lifelong/","section":"publication","summary":"Build an event detection model that expands its ontology dynamically forever.","tags":null,"title":"Lifelong Event Detection with Knowledge Transfer","type":"publication"},{"authors":["Xiaodan Hu","Pengfei Yu","Kevin Knight","Heng Ji","Bo Li","Honghui Shi"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"c2f0fce9c3de5813638ba4d7e1d7a572","permalink":"https://perfec-yu.github.io/publication/xiaodan-2021-muse/","publishdate":"2022-06-25T20:43:20.044556Z","relpermalink":"/publication/xiaodan-2021-muse/","section":"publication","summary":"","tags":null,"title":"MUSE: Textual Attributes Guided Portrait Painting Generation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://perfec-yu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Xu Han","Hao Zhu","Pengfei Yu","Ziyun Wang","Yuan Yao","Zhiyuan Liu","Maosong Sun"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"83dd5e9587c1af50cf495437dbbae9ea","permalink":"https://perfec-yu.github.io/publication/han-etal-2018-fewrel/","publishdate":"2022-06-25T20:43:20.045135Z","relpermalink":"/publication/han-etal-2018-fewrel/","section":"publication","summary":"We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.","tags":null,"title":"FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation","type":"publication"},{"authors":["Xu Han","Pengfei Yu","Zhiyuan Liu","Maosong Sun","Peng Li"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"5dec288ac7b6a412a6d795603aaadf35","permalink":"https://perfec-yu.github.io/publication/han-etal-2018-hierarchical/","publishdate":"2022-06-25T20:43:20.045908Z","relpermalink":"/publication/han-etal-2018-hierarchical/","section":"publication","summary":"Distantly supervised relation extraction employs existing knowledge graphs to automatically collect training data. While distant supervision is effective to scale relation extraction up to large-scale corpora, it inevitably suffers from the wrong labeling problem. Many efforts have been devoted to identifying valid instances from noisy data. However, most existing methods handle each relation in isolation, regardless of rich semantic correlations located in relation hierarchies. In this paper, we aim to incorporate the hierarchical information of relations for distantly supervised relation extraction and propose a novel hierarchical attention scheme. The multiple layers of our hierarchical attention scheme provide coarse-to-fine granularity to better identify valid instances, which is especially effective for extracting those long-tail relations. The experimental results on a large-scale benchmark dataset demonstrate that our models are capable of modeling the hierarchical information of relations and significantly outperform other baselines. The source code of this paper can be obtained from r̆lhttps://github.com/thunlp/HNRE.","tags":null,"title":"Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention","type":"publication"},{"authors":["Yinpei Dai","Zhijian Ou","Dawei Ren","Pengfei Yu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c5d5ed7eac96896b9f682360be5fdc59","permalink":"https://perfec-yu.github.io/publication/dai-2018-tracking/","publishdate":"2022-06-25T20:43:20.046658Z","relpermalink":"/publication/dai-2018-tracking/","section":"publication","summary":"","tags":null,"title":"Tracking of Enriched Dialog States for Flexible Conversational Information Access","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://perfec-yu.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://perfec-yu.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]